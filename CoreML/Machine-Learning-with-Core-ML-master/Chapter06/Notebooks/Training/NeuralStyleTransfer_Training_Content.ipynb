{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Machine Learning with CoreML](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-core-ml)\n",
    "**By:** Joshua Newnham (Author)  \n",
    "**Publisher:** [Packt Publishing](https://www.packtpub.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Fast Neural Style Transfer \n",
    "This notebook is concerned with extracting the **content** from an image and using this to *steer* the network (loss function). \n",
    "\n",
    "At a highlevel; this is achieved by using a model ([VGG16](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)) that has been trained to perform object recognition. The features it learns is classify the object within the image is what we use for both style and content.  \n",
    "\n",
    "The model is made up of a series of convolutional layers where these layers establish **feature maps** that can be seen as the models internal representation of the image content. Typically; the shallow layers represent basic shapes but deeper layers represent more abstract features (as they operate on a layer scale and thus have a higher-level representation of the image). The image below illustrates the *features* of an image which are activated at each of the layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/layer_activations.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore; to compare our generated image to the content image we can extract features vectors from the deeper layers and calculate a distance (with the goal of nearing 0). The image below illustrates this process and is the purpose of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/content_loss.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range, input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Re-create VGG16; replacing MaxPooling with AveragePooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_AvgPool(shape):\n",
    "    vgg16 = VGG16(input_shape=shape, weights='imagenet', include_top=False)\n",
    "    \n",
    "    avg_vgg16 = Sequential() \n",
    "    for layer in vgg16.layers:\n",
    "        if layer.__class__ == MaxPooling2D:\n",
    "            avg_vgg16.add(AveragePooling2D())\n",
    "        else:\n",
    "            avg_vgg16.add(layer) \n",
    "            \n",
    "    return avg_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_Cutoff(shape, num_convs):\n",
    "    \"\"\"\n",
    "    There are 13 convolutions in total, \n",
    "    we can choose any of them for our output \n",
    "    \"\"\"\n",
    "    vgg = VGG16_AvgPool(shape)\n",
    "    vgg16_cutoff = Sequential() \n",
    "    n = 0 \n",
    "    for layer in vgg.layers:                \n",
    "        vgg16_cutoff.add(layer)\n",
    "        \n",
    "        if layer.__class__ == Conv2D:\n",
    "            n += 1\n",
    "            if n >= num_convs:\n",
    "                break \n",
    "                \n",
    "    return vgg16_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpreprocess(img):\n",
    "    img[...,0] += 103.939\n",
    "    img[...,1] += 116.779\n",
    "    img[...,2] += 126.68\n",
    "    \n",
    "    img = img[...,::-1]\n",
    "    \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img(img):\n",
    "    img = img - img.min() \n",
    "    img = img / img.max() \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(img):\n",
    "    \"\"\"\n",
    "    Input is (H, W, C) (C = # feature maps);\n",
    "    we first need to convert it to HW, C    \n",
    "    \"\"\"\n",
    "    X = K.batch_flatten(K.permute_dimensions(img, (2, 0, 1)))\n",
    "    \n",
    "    # Now calculate the gram matrix \n",
    "    # gram = XX^T / N\n",
    "    # The constant is not important since we'll be weighting these \n",
    "    G = K.dot(X, K.transpose(X)) / img.get_shape().num_elements() \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(y, t):\n",
    "    \"\"\"\n",
    "    y: generated image \n",
    "    t: target image \n",
    "    \"\"\"\n",
    "    return K.mean(K.square(gram_matrix(y) - gram_matrix(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(fn, epochs, batch_shape):\n",
    "    t0 = datetime.now() \n",
    "    losses = [] \n",
    "    # initilise our generated image with random values \n",
    "    x = np.random.randn(np.prod(batch_shape))\n",
    "    for i in range(epochs):\n",
    "        x, l, _ = fmin_l_bfgs_b(\n",
    "            func=fn, \n",
    "            x0=x, \n",
    "            maxfun=20)\n",
    "        x = np.clip(x, -127, 127)\n",
    "\n",
    "        print(\"iteration {} loss {}\".format(i, l))\n",
    "        losses.append(l)\n",
    "        \n",
    "    t1 = datetime.now() \n",
    "    print(\"duration: {}\".format(t1-t0))\n",
    "    plt.plot(losses)\n",
    "    plt.show() \n",
    "    \n",
    "    output_img = x.reshape(*batch_shape)\n",
    "    output_img = unpreprocess(output_img)\n",
    "    return output_img[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(img_path):\n",
    "    img = image.load_img(img_path)\n",
    "    \n",
    "    # convert image to array and preprocess for vgg \n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    # grab the shape \n",
    "    batch_shape = x.shape \n",
    "    shape = x.shape[1:]\n",
    "        \n",
    "    # lets take the first convolution of each block \n",
    "    # to be the target outputs     \n",
    "    vgg = VGG16_AvgPool(shape)\n",
    "    \n",
    "    # Note: you need to select the output at index 1, since the \n",
    "    # output at index 0 corrosponds to the original vgg with maxpool \n",
    "    symbloic_conv_outputs = [\n",
    "        layer.get_output_at(1) for layer in vgg.layers if layer.name.endswith('conv1')\n",
    "    ]\n",
    "    \n",
    "    # Pick the earlier layers for more \"localised\" representaiton; \n",
    "    #Â this is the opposute to the content model where the \n",
    "    # later layers represent a more \"global\" structure \n",
    "    \n",
    "    # symbloic_conv_outputs = symbloic_conv_outputs[:2] # example of a subset \n",
    "    \n",
    "    # Make a big model that outputs multiple output layers \n",
    "    multi_output_model = Model(vgg.input, symbloic_conv_outputs)\n",
    "    \n",
    "    # calcualte the targets that are outputs for each layer \n",
    "    style_layer_outputs = [K.variable(y) for y in multi_output_model.predict(x)]\n",
    "    \n",
    "    # calculate the total style loss \n",
    "    loss = 0 \n",
    "    for symbolic, actual in zip(symbloic_conv_outputs, style_layer_outputs):\n",
    "        # gram_matrix() expects a (H, W, C) as input \n",
    "        loss += style_loss(symbolic[0], actual[0])\n",
    "        \n",
    "    grads = K.gradients(loss, multi_output_model.input)\n",
    "    \n",
    "    get_loss_and_grads = K.function(\n",
    "        inputs=[multi_output_model.input], \n",
    "        outputs=[loss] + grads)        \n",
    "    \n",
    "    def get_loss_and_grads_wrapper(x_vec):\n",
    "        \"\"\"\n",
    "        scipy's minimizer allows us to pass back \n",
    "        function value f(x) and its gradient f'(x) \n",
    "        simultaneously rather than using the fprime arg \n",
    "        \n",
    "        We cannot use get_loss_and_grads() directly, \n",
    "        the minimizer func must be a 1-D array. \n",
    "        Input to get_loss_and_grads must be [batch_of_images]\n",
    "        \n",
    "        Gradient must also be a 1-D array and both, \n",
    "        loss and graident, must be np.float64 otherwise we will \n",
    "        get an error\n",
    "        \"\"\"\n",
    "        \n",
    "        l, g = get_loss_and_grads([x_vec.reshape(*batch_shape)])\n",
    "        return l.astype(np.float64), g.flatten().astype(np.float64)\n",
    "        \n",
    "    final_img = minimize(get_loss_and_grads_wrapper, 10, batch_shape)        \n",
    "    plt.imshow(scale_img(final_img))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_IMAGE = \"../images/Van_Gogh-Starry_Night.jpg\"\n",
    "process(STYLE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
